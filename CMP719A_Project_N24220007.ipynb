{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***DOWNLOADING DEPENDENCIES***"
      ],
      "metadata": {
        "id": "c6bAElk658OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python opencv-python-headless matplotlib tqdm kagglehub\n",
        "!pip install ultralytics\n",
        "!pip install huggingface_hub ultralytics\n",
        "!pip install scrfd --quiet\n",
        "!pip install scrfd\n",
        "!pip install -q onnxruntime opencv-python-headless numpy tqdm\n",
        "!pip install -q insightface onnxruntime opencv-python-headless tqdm\n",
        "!pip install -q onnxruntime\n",
        "!pip install insightface\n",
        "!pip install onnxruntime opencv-python\n",
        "!git clone https://github.com/yakhyo/facial-analysis.git\n",
        "!bash facial-analysis/download.sh\n",
        "!wget -O scrfd_500m.onnx \"https://sourceforge.net/projects/insightface.mirror/files/v0.7/scrfd_person_2.5g.onnx/download\"\n",
        "!wget -O scrfd_10g_bnkps.onnx \\\n",
        "  https://huggingface.co/lithiumice/insightface/resolve/main/models/antelopev2/scrfd_10g_bnkps.onnx"
      ],
      "metadata": {
        "id": "MXxLh3SJ5k41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DOWNLOADING MODELS***\n",
        "1. YOLOv8n-FACE\n",
        "2. SCRFD"
      ],
      "metadata": {
        "id": "9f_ueMAG7Tvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading pre-trained YOLOv8n-Face Model\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"arnabdhar/YOLOv8-Face-Detection\",\n",
        "    filename=\"model.pt\"\n",
        ")\n",
        "print(\"Model downloaded to:\", model_path)\n",
        "\n",
        "model = YOLO(model_path)\n",
        "print(\"Model architecture loaded successfully!\")"
      ],
      "metadata": {
        "id": "eTEu4KMj7bYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DOWNLOADING DATASETS***\n",
        "1. FDDB\n",
        "2. WIDERFACE\n",
        "3. CELEBA"
      ],
      "metadata": {
        "id": "z6awxofb5y0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"cormacwc/fddb-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "wWZ8BP995xHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "src_path = kagglehub.dataset_download(\"mksaad/wider-face-a-face-detection-benchmark\")\n",
        "print(\"Downloaded to:\", src_path)\n",
        "\n",
        "# Writable destination path\n",
        "dest_path = \"/kaggle/working/wider-face-dataset\"\n",
        "os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "# Copy contents to working dir\n",
        "shutil.copytree(src_path, dest_path, dirs_exist_ok=True)\n",
        "\n",
        "print(\"Dataset copied to:\", dest_path)"
      ],
      "metadata": {
        "id": "7YnXoR4V7DcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "kfy30OM5-C49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***--FDDB DATASET--***\n",
        "\n",
        "**Haar Cascades on FDDB**\n",
        "\n"
      ],
      "metadata": {
        "id": "EXf93-pT6jll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FDDB Face Detection with Haar Cascades + Accuracy Evaluation\n",
        "\n",
        "# STEP1: Set up paths for already uploaded FDDB dataset\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Set the base paths for images and folds\n",
        "base_folder = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "folds_path = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# STEP 2: Helper functions\n",
        "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image not found: {path}\")\n",
        "    return img\n",
        "\n",
        "def detect_faces_haar_combined(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    frontal_faces = frontal_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "    profile_faces = profile_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "    flipped = cv2.flip(gray, 1)\n",
        "    flipped_profiles = profile_cascade.detectMultiScale(flipped, 1.1, 5)\n",
        "    fixed_profiles = [(gray.shape[1] - x - w, y, w, h) for (x, y, w, h) in flipped_profiles]\n",
        "    return list(frontal_faces) + list(profile_faces) + fixed_profiles\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "def ellipse_to_bbox(major, minor, angle, cx, cy, _):\n",
        "    x1 = int(cx - major)\n",
        "    y1 = int(cy - minor)\n",
        "    w = int(2 * major)\n",
        "    h = int(2 * minor)\n",
        "    return [x1, y1, w, h]\n",
        "\n",
        "# STEP 3: Load GT annotations\n",
        "gt_annotations = {}\n",
        "for i in range(1, 11):\n",
        "    with open(f\"{folds_path}/FDDB-fold-{i:02d}-ellipseList.txt\") as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            count = int(f.readline())\n",
        "            boxes = []\n",
        "            for _ in range(count):\n",
        "                vals = list(map(float, f.readline().strip().split()))\n",
        "                boxes.append(ellipse_to_bbox(*vals))\n",
        "            filename = name + \".jpg\"\n",
        "            gt_annotations[filename] = boxes\n",
        "\n",
        "# STEP 4: Detection + Evaluation\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in tqdm(files):\n",
        "        if file.endswith('.jpg'):\n",
        "            relative_path = os.path.relpath(os.path.join(root, file), base_folder)\n",
        "            key = relative_path.replace(\"\\\\\", \"/\")\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = load_image(img_path)\n",
        "            preds = detect_faces_haar_combined(img)\n",
        "            gt = gt_annotations.get(key, [])\n",
        "            matched = set()\n",
        "            for p in preds:\n",
        "                matched_flag = False\n",
        "                for i, g in enumerate(gt):\n",
        "                    if i in matched:\n",
        "                        continue\n",
        "                    if iou(p, g) >= IOU_THRESH:\n",
        "                        TP += 1\n",
        "                        matched.add(i)\n",
        "                        matched_flag = True\n",
        "                        break\n",
        "                if not matched_flag:\n",
        "                    FP += 1\n",
        "            FN += len(gt) - len(matched)\n",
        "\n",
        "# STEP 5: Report Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "KGHcwR1X6L21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Haar Cascades (with NMS + Tuned Parameters) on FDDB**"
      ],
      "metadata": {
        "id": "5Wh27YTz6sGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FDDB Face Detection with Haar Cascades + Accuracy Evaluation (with NMS + Tuned Parameters)\n",
        "\n",
        "# STEP 1: Set up paths for already uploaded FDDB dataset\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Set the base paths for images and folds\n",
        "base_folder = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "folds_path = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# STEP 2: Helper functions\n",
        "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image not found: {path}\")\n",
        "    return img\n",
        "\n",
        "def non_max_suppression_fast(boxes, overlapThresh):\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    boxes = np.array(boxes)\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "\n",
        "    pick = []\n",
        "    x1 = boxes[:,0]\n",
        "    y1 = boxes[:,1]\n",
        "    x2 = boxes[:,0] + boxes[:,2]\n",
        "    y2 = boxes[:,1] + boxes[:,3]\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(y2)\n",
        "\n",
        "    while len(idxs) > 0:\n",
        "        last = idxs[-1]\n",
        "        pick.append(last)\n",
        "        xx1 = np.maximum(x1[last], x1[idxs[:-1]])\n",
        "        yy1 = np.maximum(y1[last], y1[idxs[:-1]])\n",
        "        xx2 = np.minimum(x2[last], x2[idxs[:-1]])\n",
        "        yy2 = np.minimum(y2[last], y2[idxs[:-1]])\n",
        "\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "        overlap = (w * h) / area[idxs[:-1]]\n",
        "        idxs = np.delete(idxs, np.concatenate(([len(idxs) - 1], np.where(overlap > overlapThresh)[0])))\n",
        "\n",
        "    return boxes[pick].astype(\"int\")\n",
        "\n",
        "def detect_faces_haar_combined(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    frontal_faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=6, minSize=(30, 30))\n",
        "    profile_faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=6, minSize=(30, 30))\n",
        "    flipped = cv2.flip(gray, 1)\n",
        "    flipped_profiles = profile_cascade.detectMultiScale(flipped, scaleFactor=1.2, minNeighbors=6, minSize=(30, 30))\n",
        "    fixed_profiles = [(gray.shape[1] - x - w, y, w, h) for (x, y, w, h) in flipped_profiles]\n",
        "    all_faces = list(frontal_faces) + list(profile_faces) + fixed_profiles\n",
        "    return non_max_suppression_fast(all_faces, 0.3)\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "def ellipse_to_bbox(major, minor, angle, cx, cy, unused):\n",
        "    x1 = int(cx - major)\n",
        "    y1 = int(cy - minor)\n",
        "    w = int(2 * major)\n",
        "    h = int(2 * minor)\n",
        "    return [x1, y1, w, h]\n",
        "\n",
        "# STEP 3: Load GT annotations\n",
        "gt_annotations = {}\n",
        "for i in range(1, 11):\n",
        "    with open(f\"{folds_path}/FDDB-fold-{i:02d}-ellipseList.txt\") as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            count = int(f.readline())\n",
        "            boxes = []\n",
        "            for _ in range(count):\n",
        "                vals = list(map(float, f.readline().strip().split()))\n",
        "                boxes.append(ellipse_to_bbox(*vals))\n",
        "            filename = name + \".jpg\"\n",
        "            gt_annotations[filename] = boxes\n",
        "\n",
        "# STEP 4: Detection + Evaluation\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in tqdm(files):\n",
        "        if file.endswith('.jpg'):\n",
        "            relative_path = os.path.relpath(os.path.join(root, file), base_folder)\n",
        "            key = relative_path.replace(\"\\\\\", \"/\")\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = load_image(img_path)\n",
        "            preds = detect_faces_haar_combined(img)\n",
        "            gt = gt_annotations.get(key, [])\n",
        "            matched = set()\n",
        "            for p in preds:\n",
        "                matched_flag = False\n",
        "                for i, g in enumerate(gt):\n",
        "                    if i in matched:\n",
        "                        continue\n",
        "                    if iou(p, g) >= IOU_THRESH:\n",
        "                        TP += 1\n",
        "                        matched.add(i)\n",
        "                        matched_flag = True\n",
        "                        break\n",
        "                if not matched_flag:\n",
        "                    FP += 1\n",
        "            FN += len(gt) - len(matched)\n",
        "\n",
        "# STEP 5: Report Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "zz-pM4sk6stx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOV8-FACE on FDDB**"
      ],
      "metadata": {
        "id": "ClcDD9CK76j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FDDB DATASET RUN WITH YOLOV8-FACE MODEL:\n",
        "\n",
        "# STEP 1: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 2: Set FDDB paths\n",
        "base_folder = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "folds_path = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# Load model from HuggingFace download path\n",
        "model_path = \"/root/.cache/huggingface/hub/models--arnabdhar--YOLOv8-Face-Detection/snapshots/52fa54977207fa4f021de949b515fb19dcab4488/model.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# STEP 3: Load Ground Truth Ellipses â†’ Convert to BBoxes\n",
        "def ellipse_to_bbox(major, minor, angle, cx, cy, score=None):\n",
        "    x1 = int(cx - major)\n",
        "    y1 = int(cy - minor)\n",
        "    w = int(2 * major)\n",
        "    h = int(2 * minor)\n",
        "    return [x1, y1, w, h]\n",
        "\n",
        "gt_annotations = {}\n",
        "for i in range(1, 11):\n",
        "    with open(f\"{folds_path}/FDDB-fold-{i:02d}-ellipseList.txt\") as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            count = int(f.readline())\n",
        "            boxes = []\n",
        "            for _ in range(count):\n",
        "                vals = list(map(float, f.readline().strip().split()))\n",
        "                boxes.append(ellipse_to_bbox(*vals))\n",
        "            filename = name + \".jpg\"\n",
        "            gt_annotations[filename] = boxes\n",
        "\n",
        "# STEP 4: Define IoU\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# STEP 5: Evaluate on FDDB\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in tqdm(files):\n",
        "        if file.endswith('.jpg'):\n",
        "            rel_path = os.path.relpath(os.path.join(root, file), base_folder)\n",
        "            key = rel_path.replace(\"\\\\\", \"/\")\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Run YOLOv8 inference\n",
        "            results = model.predict(img, verbose=False)\n",
        "            preds = []\n",
        "            for r in results:\n",
        "                for box in r.boxes:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    preds.append([int(x1), int(y1), int(w), int(h)])\n",
        "\n",
        "            gt = gt_annotations.get(key, [])\n",
        "            matched = set()\n",
        "\n",
        "            for p in preds:\n",
        "                matched_flag = False\n",
        "                for i, g in enumerate(gt):\n",
        "                    if i in matched:\n",
        "                        continue\n",
        "                    if iou(p, g) >= IOU_THRESH:\n",
        "                        TP += 1\n",
        "                        matched.add(i)\n",
        "                        matched_flag = True\n",
        "                        break\n",
        "                if not matched_flag:\n",
        "                    FP += 1\n",
        "            FN += len(gt) - len(matched)\n",
        "\n",
        "# STEP 6: Compute Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"YOLOv8-Face Evaluation Results on FDDB:\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "R-sAyWaG78Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOv8-Face (Improved with confidence tuning and filtering) on FDDB**"
      ],
      "metadata": {
        "id": "WXrD-ilm8KvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FDDB DATASET RUN WITH YOLOv8-FACE MODEL (Improved with confidence tuning and filtering):\n",
        "\n",
        "# STEP 1: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 2: Set FDDB paths\n",
        "base_folder = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "folds_path = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# Load model from HuggingFace download path\n",
        "model_path = \"/root/.cache/huggingface/hub/models--arnabdhar--YOLOv8-Face-Detection/snapshots/52fa54977207fa4f021de949b515fb19dcab4488/model.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# STEP 3: Load Ground Truth Ellipses â†’ Convert to BBoxes\n",
        "def ellipse_to_bbox(major, minor, angle, cx, cy, score=None):\n",
        "    x1 = int(cx - major)\n",
        "    y1 = int(cy - minor)\n",
        "    w = int(2 * major)\n",
        "    h = int(2 * minor)\n",
        "    return [x1, y1, w, h]\n",
        "\n",
        "gt_annotations = {}\n",
        "for i in range(1, 11):\n",
        "    with open(f\"{folds_path}/FDDB-fold-{i:02d}-ellipseList.txt\") as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            count = int(f.readline())\n",
        "            boxes = []\n",
        "            for _ in range(count):\n",
        "                vals = list(map(float, f.readline().strip().split()))\n",
        "                boxes.append(ellipse_to_bbox(*vals))\n",
        "            filename = name + \".jpg\"\n",
        "            gt_annotations[filename] = boxes\n",
        "\n",
        "# STEP 4: Define IoU\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# STEP 5: Evaluate on FDDB\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "CONF_THRESH = 0.4\n",
        "MIN_BOX_SIZE = 20\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in tqdm(files):\n",
        "        if file.endswith('.jpg'):\n",
        "            rel_path = os.path.relpath(os.path.join(root, file), base_folder)\n",
        "            key = rel_path.replace(\"\\\\\", \"/\")\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Run YOLOv8 inference with adjusted confidence threshold\n",
        "            results = model.predict(img, conf=CONF_THRESH, verbose=False)\n",
        "            preds = []\n",
        "            for r in results:\n",
        "                for box in r.boxes:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    if w > MIN_BOX_SIZE and h > MIN_BOX_SIZE:\n",
        "                        preds.append([int(x1), int(y1), int(w), int(h)])\n",
        "\n",
        "            gt = gt_annotations.get(key, [])\n",
        "            matched = set()\n",
        "\n",
        "            for p in preds:\n",
        "                matched_flag = False\n",
        "                for i, g in enumerate(gt):\n",
        "                    if i in matched:\n",
        "                        continue\n",
        "                    if iou(p, g) >= IOU_THRESH:\n",
        "                        TP += 1\n",
        "                        matched.add(i)\n",
        "                        matched_flag = True\n",
        "                        break\n",
        "                if not matched_flag:\n",
        "                    FP += 1\n",
        "            FN += len(gt) - len(matched)\n",
        "\n",
        "# STEP 6: Compute Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"YOLOv8-Face Evaluation Results on FDDB:\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "AJPr43x68Omp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOv8-Face (Confidence Filtering + Optional NMS) on FDDB**"
      ],
      "metadata": {
        "id": "iIdpXuBT8oac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved FDDB Evaluation with YOLOv8-Face: Confidence Filtering + Optional NMS\n",
        "\n",
        "# STEP 1: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 2: Set FDDB paths\n",
        "base_folder = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "folds_path = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# Load YOLOv8-Face model from HuggingFace\n",
        "model_path = \"/root/.cache/huggingface/hub/models--arnabdhar--YOLOv8-Face-Detection/snapshots/52fa54977207fa4f021de949b515fb19dcab4488/model.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# STEP 3: Ground Truth Ellipse â†’ BBox\n",
        "def ellipse_to_bbox(major, minor, angle, cx, cy, score=None):\n",
        "    x1 = int(cx - major)\n",
        "    y1 = int(cy - minor)\n",
        "    w = int(2 * major)\n",
        "    h = int(2 * minor)\n",
        "    return [x1, y1, w, h]\n",
        "\n",
        "gt_annotations = {}\n",
        "for i in range(1, 11):\n",
        "    with open(f\"{folds_path}/FDDB-fold-{i:02d}-ellipseList.txt\") as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            count = int(f.readline())\n",
        "            boxes = []\n",
        "            for _ in range(count):\n",
        "                vals = list(map(float, f.readline().strip().split()))\n",
        "                boxes.append(ellipse_to_bbox(*vals))\n",
        "            filename = name + \".jpg\"\n",
        "            gt_annotations[filename] = boxes\n",
        "\n",
        "# STEP 4: IoU calculation\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# STEP 5: Evaluate with confidence threshold and optional size filtering\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "CONF_THRESH = 0.3\n",
        "MIN_SIZE = 20  # Filter out small detections\n",
        "\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in tqdm(files):\n",
        "        if file.endswith('.jpg'):\n",
        "            rel_path = os.path.relpath(os.path.join(root, file), base_folder)\n",
        "            key = rel_path.replace(\"\\\\\", \"/\")\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            results = model.predict(img, conf=CONF_THRESH, verbose=False)\n",
        "            preds = []\n",
        "            for r in results:\n",
        "                for box in r.boxes:\n",
        "                    conf = box.conf[0].item()\n",
        "                    if conf < CONF_THRESH:\n",
        "                        continue\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    if w < MIN_SIZE or h < MIN_SIZE:\n",
        "                        continue\n",
        "                    preds.append([int(x1), int(y1), int(w), int(h)])\n",
        "\n",
        "            gt = gt_annotations.get(key, [])\n",
        "            matched = set()\n",
        "\n",
        "            for p in preds:\n",
        "                matched_flag = False\n",
        "                for i, g in enumerate(gt):\n",
        "                    if i in matched:\n",
        "                        continue\n",
        "                    if iou(p, g) >= IOU_THRESH:\n",
        "                        TP += 1\n",
        "                        matched.add(i)\n",
        "                        matched_flag = True\n",
        "                        break\n",
        "                if not matched_flag:\n",
        "                    FP += 1\n",
        "            FN += len(gt) - len(matched)\n",
        "\n",
        "# STEP 6: Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"YOLOv8-Face Evaluation Results on FDDB:\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "W1XI75FT8dqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCRFD on FDDB**"
      ],
      "metadata": {
        "id": "gPCiMtyW82jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "# STEP 1: Model Setup\n",
        "model_dir = os.path.expanduser(\"~/.insightface/models/antelopev2\")\n",
        "model_file = os.path.join(model_dir, \"scrfd_10g_bnkps.onnx\")\n",
        "\n",
        "if not os.path.exists(model_file):\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    !wget -O \"{model_file}\" https://huggingface.co/lithiumice/insightface/resolve/main/models/antelopev2/scrfd_10g_bnkps.onnx\n",
        "\n",
        "app = FaceAnalysis(name=\"antelopev2\", providers=['CPUExecutionProvider'])\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "# STEP 2: Dataset Paths\n",
        "IMG_DIR = \"/kaggle/input/fddb-dataset/FDDB copy/originalPics\"\n",
        "FOLD_DIR = \"/kaggle/input/fddb-dataset/FDDB copy/FDDB-folds\"\n",
        "\n",
        "# STEP 3: Ground Truth Loading\n",
        "def ellipse_to_bbox(a, b, ang, cx, cy, *_):\n",
        "    return [int(cx - a), int(cy - b), int(2 * a), int(2 * b)]\n",
        "\n",
        "gt = {}\n",
        "for i in range(1, 11):\n",
        "    path = os.path.join(FOLD_DIR, f\"FDDB-fold-{i:02d}-ellipseList.txt\")\n",
        "    with open(path) as f:\n",
        "        while True:\n",
        "            name = f.readline().strip()\n",
        "            if not name:\n",
        "                break\n",
        "            n = int(f.readline().strip())\n",
        "            boxes = []\n",
        "            for _ in range(n):\n",
        "                parts = list(map(float, f.readline().split()))\n",
        "                boxes.append(ellipse_to_bbox(*parts))\n",
        "            img_name = os.path.basename(name) + \".jpg\"\n",
        "            gt[img_name] = boxes\n",
        "\n",
        "# STEP 4: IoU Function\n",
        "def iou(a, b):\n",
        "    xA = max(a[0], b[0])\n",
        "    yA = max(a[1], b[1])\n",
        "    xB = min(a[0] + a[2], b[0] + b[2])\n",
        "    yB = min(a[1] + a[3], b[1] + b[3])\n",
        "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "    union = a[2] * a[3] + b[2] * b[3] - inter\n",
        "    return inter / union if union > 0 else 0\n",
        "\n",
        "# STEP 5: Evaluate\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.3\n",
        "\n",
        "for root, dirs, files in os.walk(IMG_DIR):\n",
        "    for fn in tqdm(files):\n",
        "        if not fn.endswith(\".jpg\"):\n",
        "            continue\n",
        "\n",
        "        img_path = os.path.join(root, fn)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Run SCRFD detection\n",
        "        faces = app.get(img)\n",
        "        preds = [list(map(int, face.bbox)) for face in faces if face.det_score > 0.5]\n",
        "\n",
        "        # Match predictions with ground truth\n",
        "        gts = gt.get(fn, [])  # Match by filename only\n",
        "        matched = set()\n",
        "        for p in preds:\n",
        "            hit = False\n",
        "            for idx, g in enumerate(gts):\n",
        "                if idx in matched:\n",
        "                    continue\n",
        "                if iou(p, g) >= IOU_THRESH:\n",
        "                    TP += 1\n",
        "                    matched.add(idx)\n",
        "                    hit = True\n",
        "                    break\n",
        "            if not hit:\n",
        "                FP += 1\n",
        "        FN += len(gts) - len(matched)\n",
        "\n",
        "\n",
        "\n",
        "# STEP 6:\n",
        "precision = TP / (TP + FP + 1e-9)\n",
        "recall = TP / (TP + FN + 1e-9)\n",
        "f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "\n",
        "print(\"âœ… SCRFD on FDDB Results:\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "hz0jGJh482KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***--WIDER FACE  DATASET--***\n",
        "\n",
        "**HAAR CASCADES (FRONTAL ONLY) on WIDER FACE**"
      ],
      "metadata": {
        "id": "t1Rjch1L96Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HAAR CASCADES (FRONTAL ONLY)\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "img_base_dir = \"/kaggle/working/wider-face-dataset/WIDER_val/WIDER_val/images\"\n",
        "save_dir = \"/kaggle/working/wider_haar_preds\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load Haar Cascade frontal face detector\n",
        "haar_model = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Parameters\n",
        "scaleFactor = 1.1\n",
        "minNeighbors = 4\n",
        "minSize = (30, 30)\n",
        "\n",
        "# Collect image paths\n",
        "img_paths = sorted(glob.glob(f\"{img_base_dir}/*/*.jpg\"))\n",
        "\n",
        "# Run face detection\n",
        "for img_path in tqdm(img_paths):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    detections = haar_model.detectMultiScale(gray, scaleFactor=scaleFactor,\n",
        "                                             minNeighbors=minNeighbors, minSize=minSize)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    txt_path = os.path.join(save_dir, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "\n",
        "    with open(txt_path, \"w\") as f:\n",
        "        for (x, y, w_box, h_box) in detections:\n",
        "            if w_box < 10 or h_box < 10:\n",
        "                continue  # Ignore very small boxes\n",
        "            x_center = (x + w_box / 2) / w\n",
        "            y_center = (y + h_box / 2) / h\n",
        "            w_norm = w_box / w\n",
        "            h_norm = h_box / h\n",
        "            f.write(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
        "\n",
        "# Evaluate Haar predictions\n",
        "tp, fp, fn, precision, recall, f1 = compute_metrics(\"/kaggle/working/wider_haar_preds\", label_save_dir)\n",
        "\n",
        "print(\"ðŸ” Haar Cascade Evaluation on WIDER FACE\")\n",
        "print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "JpLNqIKO-a7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HAAR CASCADES (FRONTAL + SIDE) + NMS on WIDER FACE**"
      ],
      "metadata": {
        "id": "kW87Eca_-8tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HAAR CASCADES (FRONTAL + SIDE) + NMS\n",
        "\n",
        "# STEP 1: Install required packages\n",
        "!pip install opencv-python-headless matplotlib tqdm\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# STEP 3: Set paths (adjust according to your mount point if needed)\n",
        "IMG_DIR = \"/kaggle/working/wider-face-dataset/WIDER_train/WIDER_train/images\"\n",
        "ANNOT_FILE = \"/kaggle/working/wider-face-dataset/wider_face_split/wider_face_split/wider_face_train_bbx_gt.txt\"\n",
        "\n",
        "# STEP 4: Load Haar Cascades\n",
        "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
        "\n",
        "# STEP 5: Load Ground Truth Annotations\n",
        "gt = defaultdict(list)\n",
        "\n",
        "with open(ANNOT_FILE) as f:\n",
        "    lines = iter(f.readlines())\n",
        "    for img_path in lines:\n",
        "        img_path = img_path.strip()\n",
        "        face_count = int(next(lines).strip())\n",
        "        for _ in range(face_count):\n",
        "            x, y, w, h, *_ = map(int, next(lines).strip().split())\n",
        "            gt[img_path].append([x, y, w, h])\n",
        "\n",
        "# STEP 6: IOU + NMS functions\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "def non_max_suppression(boxes, iou_threshold=0.3):\n",
        "    if not boxes:\n",
        "        return []\n",
        "    boxes = np.array(boxes)\n",
        "    x1 = boxes[:,0]\n",
        "    y1 = boxes[:,1]\n",
        "    x2 = boxes[:,0] + boxes[:,2]\n",
        "    y2 = boxes[:,1] + boxes[:,3]\n",
        "    areas = boxes[:,2] * boxes[:,3]\n",
        "    order = areas.argsort()[::-1]\n",
        "    keep = []\n",
        "\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0, xx2 - xx1)\n",
        "        h = np.maximum(0, yy2 - yy1)\n",
        "        overlap = (w * h) / (areas[i] + areas[order[1:]] - (w * h))\n",
        "        order = order[1:][overlap <= iou_threshold]\n",
        "    return boxes[keep].tolist()\n",
        "\n",
        "# STEP 7: Detection + Evaluation\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.5\n",
        "\n",
        "limited_gt = dict(list(gt.items())[:1000])\n",
        "for rel_path, true_boxes in tqdm(limited_gt.items(), total=len(limited_gt)):\n",
        "    full_path = os.path.join(IMG_DIR, rel_path)\n",
        "    img = cv2.imread(full_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect frontal and profile faces\n",
        "    frontal = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "    profile = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Flip image to catch opposite profiles\n",
        "    flipped = cv2.flip(gray, 1)\n",
        "    flipped_profiles = profile_cascade.detectMultiScale(flipped, 1.1, 5)\n",
        "    flipped_profiles = [(gray.shape[1] - x - w, y, w, h) for (x, y, w, h) in flipped_profiles]\n",
        "\n",
        "    all_detections = list(frontal) + list(profile) + flipped_profiles\n",
        "    preds = non_max_suppression(all_detections, iou_threshold=0.3)\n",
        "\n",
        "    matched = set()\n",
        "    for p in preds:\n",
        "        matched_flag = False\n",
        "        for i, g in enumerate(true_boxes):\n",
        "            if i in matched:\n",
        "                continue\n",
        "            if iou(p, g) >= IOU_THRESH:\n",
        "                TP += 1\n",
        "                matched.add(i)\n",
        "                matched_flag = True\n",
        "                break\n",
        "        if not matched_flag:\n",
        "            FP += 1\n",
        "    FN += len(true_boxes) - len(matched)\n",
        "\n",
        "# STEP 8: Print final results\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(\"\\nâœ… Haar + NMS Evaluation on WIDER FACE\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "cZaTiYTV_E4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOV8n-Face (without NMS) on WIDER FACE**"
      ],
      "metadata": {
        "id": "ObygXEYV_SvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOV8n-Face run on WIDER FACE Dataset withot NMS\n",
        "\n",
        "# STEP 1: Install and import required libraries\n",
        "!pip install -q ultralytics\n",
        "from ultralytics import YOLO\n",
        "import os, glob, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# STEP 2: Define all paths\n",
        "base_dir = \"/kaggle/working/wider-face-dataset\"\n",
        "annotation_file = os.path.join(base_dir, \"wider_face_split\", \"wider_face_split\", \"wider_face_val_bbx_gt.txt\")\n",
        "img_base_dir = os.path.join(base_dir, \"WIDER_val\", \"WIDER_val\", \"images\")\n",
        "label_save_dir = \"/kaggle/working/wider_yolo_labels\"\n",
        "pred_dir = \"/kaggle/working/yolo_preds\"\n",
        "os.makedirs(label_save_dir, exist_ok=True)\n",
        "os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "# STEP 3: Convert ground-truth annotations to YOLO format\n",
        "with open(annotation_file, 'r') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "i = 0\n",
        "while i < len(lines):\n",
        "    img_rel_path = lines[i].strip()\n",
        "    n_faces = int(lines[i + 1].strip())\n",
        "    label_lines = lines[i + 2:i + 2 + n_faces]\n",
        "\n",
        "    img_path = os.path.join(img_base_dir, img_rel_path)\n",
        "    label_path = os.path.join(label_save_dir, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        i += 2 + n_faces\n",
        "        continue\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    with open(label_path, 'w') as label_file:\n",
        "        for line in label_lines:\n",
        "            x, y, width, height, *_ = map(float, line.strip().split())\n",
        "            if width < 10 or height < 10:\n",
        "                continue\n",
        "            x_center = (x + width / 2) / w\n",
        "            y_center = (y + height / 2) / h\n",
        "            w_norm = width / w\n",
        "            h_norm = height / h\n",
        "            label_file.write(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
        "    i += 2 + n_faces\n",
        "\n",
        "# STEP 4: Load YOLOv8-Face model\n",
        "model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
        "model = YOLO(model_path)\n",
        "print(\"YOLOv8-Face model loaded successfully!\")\n",
        "\n",
        "# STEP 5: Run inference on validation images\n",
        "img_paths = sorted(glob.glob(f\"{img_base_dir}/*/*.jpg\"))\n",
        "\n",
        "for img_path in tqdm(img_paths):  # Run on full set\n",
        "    result = model.predict(img_path, conf=0.15, verbose=False)[0]\n",
        "    pred_file = os.path.join(pred_dir, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "\n",
        "    with open(pred_file, 'w') as f:\n",
        "        if result.boxes is not None:\n",
        "            for box in result.boxes.xywhn.cpu().numpy():\n",
        "                x_center, y_center, w_box, h_box = box[:4]\n",
        "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {w_box:.6f} {h_box:.6f}\\n\")\n",
        "\n",
        "# STEP 5: Define evaluation functions\n",
        "def compute_iou(box1, box2):\n",
        "    def to_xyxy(box):\n",
        "        xc, yc, w, h = box\n",
        "        x1 = xc - w / 2\n",
        "        y1 = yc - h / 2\n",
        "        x2 = xc + w / 2\n",
        "        y2 = yc + h / 2\n",
        "        return [x1, y1, x2, y2]\n",
        "\n",
        "    b1 = to_xyxy(box1)\n",
        "    b2 = to_xyxy(box2)\n",
        "    xA, yA, xB, yB = max(b1[0], b2[0]), max(b1[1], b2[1]), min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1Area = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "    box2Area = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "    union = box1Area + box2Area - interArea\n",
        "    return interArea / union if union != 0 else 0\n",
        "\n",
        "def compute_metrics(pred_dir, gt_dir, iou_thresh=0.5):\n",
        "    TP = FP = FN = 0\n",
        "    pred_files = glob.glob(os.path.join(pred_dir, \"*.txt\"))\n",
        "\n",
        "    for pred_file in pred_files:\n",
        "        base = os.path.basename(pred_file)\n",
        "        gt_file = os.path.join(gt_dir, base)\n",
        "        if not os.path.exists(gt_file):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            preds = np.loadtxt(pred_file, ndmin=2)\n",
        "            gts = np.loadtxt(gt_file, ndmin=2)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        matched_gt = np.zeros(len(gts), dtype=bool)\n",
        "\n",
        "        for pred in preds:\n",
        "            best_iou, best_idx = 0, -1\n",
        "            for j, gt in enumerate(gts):\n",
        "                iou = compute_iou(pred[1:], gt[1:])\n",
        "                if iou > best_iou:\n",
        "                    best_iou, best_idx = iou, j\n",
        "\n",
        "            if best_iou >= iou_thresh and not matched_gt[best_idx]:\n",
        "                TP += 1\n",
        "                matched_gt[best_idx] = True\n",
        "            else:\n",
        "                FP += 1\n",
        "\n",
        "        FN += len(gts) - matched_gt.sum()\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-6)\n",
        "    recall = TP / (TP + FN + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    return TP, FP, FN, precision, recall, f1\n",
        "\n",
        "# STEP 6: Run evaluation if predictions exist\n",
        "if not os.path.exists(pred_dir) or len(os.listdir(pred_dir)) == 0:\n",
        "    print(f\"Prediction folder '{pred_dir}' is missing or empty. Please run inference first.\")\n",
        "else:\n",
        "    tp, fp, fn, precision, recall, f1 = compute_metrics(pred_dir, label_save_dir)\n",
        "    print(\"YOLOv8-Face Evaluation on WIDER FACE\")\n",
        "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "krRWxQZ8_YcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOV8n-Face (with NMS) on WIDER FACE**"
      ],
      "metadata": {
        "id": "a9VjRbAp_yei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOV8n-Face run on WIDER FACE Dataset with NMS\n",
        "\n",
        "# STEP 1: Install and import required libraries\n",
        "!pip install -q ultralytics\n",
        "from ultralytics import YOLO\n",
        "import os, glob, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from torchvision.ops import nms\n",
        "import torch\n",
        "\n",
        "# STEP 2: Define all paths\n",
        "base_dir = \"/kaggle/working/wider-face-dataset\"\n",
        "annotation_file = os.path.join(base_dir, \"wider_face_split\", \"wider_face_split\", \"wider_face_val_bbx_gt.txt\")\n",
        "img_base_dir = os.path.join(base_dir, \"WIDER_val\", \"WIDER_val\", \"images\")\n",
        "label_save_dir = \"/kaggle/working/wider_yolo_labels\"\n",
        "pred_dir = \"/kaggle/working/yolo_preds\"\n",
        "os.makedirs(label_save_dir, exist_ok=True)\n",
        "os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "# STEP 3: Convert ground-truth annotations to YOLO format\n",
        "with open(annotation_file, 'r') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "i = 0\n",
        "while i < len(lines):\n",
        "    img_rel_path = lines[i].strip()\n",
        "    n_faces = int(lines[i + 1].strip())\n",
        "    label_lines = lines[i + 2:i + 2 + n_faces]\n",
        "\n",
        "    img_path = os.path.join(img_base_dir, img_rel_path)\n",
        "    label_path = os.path.join(label_save_dir, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        i += 2 + n_faces\n",
        "        continue\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    with open(label_path, 'w') as label_file:\n",
        "        for line in label_lines:\n",
        "            x, y, width, height, *_ = map(float, line.strip().split())\n",
        "            if width < 10 or height < 10:\n",
        "                continue\n",
        "            x_center = (x + width / 2) / w\n",
        "            y_center = (y + height / 2) / h\n",
        "            w_norm = width / w\n",
        "            h_norm = height / h\n",
        "            label_file.write(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
        "    i += 2 + n_faces\n",
        "\n",
        "# STEP 4: Load YOLOv8-Face model\n",
        "model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
        "model = YOLO(model_path)\n",
        "print(\"\\u2705 YOLOv8-Face model loaded successfully!\")\n",
        "\n",
        "# STEP 5: Run inference with NMS\n",
        "img_paths = sorted(glob.glob(f\"{img_base_dir}/*/*.jpg\"))\n",
        "\n",
        "def apply_nms(boxes, scores, iou_threshold=0.5):\n",
        "    boxes_xyxy = []\n",
        "    for b in boxes:\n",
        "        x_c, y_c, w, h = b\n",
        "        x1 = x_c - w / 2\n",
        "        y1 = y_c - h / 2\n",
        "        x2 = x_c + w / 2\n",
        "        y2 = y_c + h / 2\n",
        "        boxes_xyxy.append([x1, y1, x2, y2])\n",
        "    boxes_tensor = torch.tensor(boxes_xyxy, dtype=torch.float32)\n",
        "    scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
        "    keep = nms(boxes_tensor, scores_tensor, iou_threshold)\n",
        "    return keep\n",
        "\n",
        "for img_path in tqdm(img_paths):\n",
        "    result = model.predict(img_path, conf=0.2, verbose=False)[0]\n",
        "    pred_file = os.path.join(pred_dir, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "\n",
        "    if result.boxes is not None and result.boxes.xywhn.numel() > 0:\n",
        "        boxes = result.boxes.xywhn.cpu().numpy()\n",
        "        scores = result.boxes.conf.cpu().numpy()\n",
        "\n",
        "        # Apply NMS\n",
        "        boxes = np.atleast_2d(boxes)\n",
        "        scores = np.atleast_1d(scores)\n",
        "        keep_indices = apply_nms(boxes, scores, iou_threshold=0.5)\n",
        "        boxes = boxes[keep_indices]\n",
        "\n",
        "        # Ensure boxes is 2D for safe iteration\n",
        "        if boxes.ndim == 1:\n",
        "            boxes = boxes[np.newaxis, :]\n",
        "    else:\n",
        "        boxes = np.empty((0, 4))\n",
        "\n",
        "    with open(pred_file, 'w') as f:\n",
        "        for box in boxes:\n",
        "            x_center, y_center, w_box, h_box = box[:4]\n",
        "            f.write(f\"0 {x_center:.6f} {y_center:.6f} {w_box:.6f} {h_box:.6f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# STEP 6: Define evaluation functions\n",
        "def compute_iou(box1, box2):\n",
        "    def to_xyxy(box):\n",
        "        xc, yc, w, h = box\n",
        "        x1 = xc - w / 2\n",
        "        y1 = yc - h / 2\n",
        "        x2 = xc + w / 2\n",
        "        y2 = yc + h / 2\n",
        "        return [x1, y1, x2, y2]\n",
        "\n",
        "    b1 = to_xyxy(box1)\n",
        "    b2 = to_xyxy(box2)\n",
        "    xA, yA, xB, yB = max(b1[0], b2[0]), max(b1[1], b2[1]), min(b1[2], b2[2]), min(b1[3], b2[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1Area = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
        "    box2Area = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
        "    union = box1Area + box2Area - interArea\n",
        "    return interArea / union if union != 0 else 0\n",
        "\n",
        "def compute_metrics(pred_dir, gt_dir, iou_thresh=0.5):\n",
        "    TP = FP = FN = 0\n",
        "    pred_files = glob.glob(os.path.join(pred_dir, \"*.txt\"))\n",
        "\n",
        "    for pred_file in pred_files:\n",
        "        base = os.path.basename(pred_file)\n",
        "        gt_file = os.path.join(gt_dir, base)\n",
        "        if not os.path.exists(gt_file):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            preds = np.loadtxt(pred_file, ndmin=2)\n",
        "            gts = np.loadtxt(gt_file, ndmin=2)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        matched_gt = np.zeros(len(gts), dtype=bool)\n",
        "\n",
        "        for pred in preds:\n",
        "            best_iou, best_idx = 0, -1\n",
        "            for j, gt in enumerate(gts):\n",
        "                iou = compute_iou(pred[1:], gt[1:])\n",
        "                if iou > best_iou:\n",
        "                    best_iou, best_idx = iou, j\n",
        "\n",
        "            if best_iou >= iou_thresh and not matched_gt[best_idx]:\n",
        "                TP += 1\n",
        "                matched_gt[best_idx] = True\n",
        "            else:\n",
        "                FP += 1\n",
        "\n",
        "        FN += len(gts) - matched_gt.sum()\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-6)\n",
        "    recall = TP / (TP + FN + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    return TP, FP, FN, precision, recall, f1\n",
        "\n",
        "# STEP 7: Run evaluation\n",
        "if not os.path.exists(pred_dir) or len(os.listdir(pred_dir)) == 0:\n",
        "    print(f\"Prediction folder '{pred_dir}' is missing or empty. Please run inference first.\")\n",
        "else:\n",
        "    tp, fp, fn, precision, recall, f1 = compute_metrics(pred_dir, label_save_dir)\n",
        "    print(\"YOLOv8-Face Evaluation on WIDER FACE\")\n",
        "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "LFmBkURJ_y5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCRFD ON WIDER FACE**"
      ],
      "metadata": {
        "id": "CWWQAWRuANpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "# STEP 1: Initialize SCRFD\n",
        "app = FaceAnalysis(name=\"antelopev2\", providers=['CPUExecutionProvider'])\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))  # Larger input size improves detection\n",
        "\n",
        "# STEP 2: Define Paths\n",
        "IMG_DIR = \"/kaggle/working/wider-face-dataset/WIDER_val/WIDER_val/images\"\n",
        "ANNOTATION_FILE = \"/kaggle/working/wider-face-dataset/wider_face_split/wider_face_split/wider_face_val_bbx_gt.txt\"\n",
        "\n",
        "# STEP 3: Parse Annotations\n",
        "gt = {}\n",
        "with open(ANNOTATION_FILE, \"r\") as f:\n",
        "    while True:\n",
        "        name = f.readline().strip()\n",
        "        if not name:\n",
        "            break\n",
        "        n = int(f.readline())\n",
        "        boxes = []\n",
        "        for _ in range(n):\n",
        "            vals = list(map(int, f.readline().strip().split()))\n",
        "            x, y, w, h = vals[:4]\n",
        "            if w > 15 and h > 15:  # Skip very small faces\n",
        "                boxes.append([x, y, w, h])\n",
        "        gt[name] = boxes\n",
        "\n",
        "# STEP 4: IoU Function\n",
        "def iou(a, b):\n",
        "    xA = max(a[0], b[0])\n",
        "    yA = max(a[1], b[1])\n",
        "    xB = min(a[0]+a[2], b[0]+b[2])\n",
        "    yB = min(a[1]+a[3], b[1]+b[3])\n",
        "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "    union = a[2]*a[3] + b[2]*b[3] - inter\n",
        "    return inter / union if union > 0 else 0\n",
        "\n",
        "# STEP 5: Evaluate on Images\n",
        "TP = FP = FN = 0\n",
        "IOU_THRESH = 0.1\n",
        "image_count = 0\n",
        "MAX_IMAGES = 3226\n",
        "\n",
        "for rel_path, gts in tqdm(gt.items(), desc=\"Evaluating images\"):\n",
        "    if image_count >= MAX_IMAGES:\n",
        "        break\n",
        "\n",
        "    full_img_path = os.path.join(IMG_DIR, rel_path)\n",
        "    if not os.path.exists(full_img_path):\n",
        "        print(f\"Missing image: {full_img_path}\")\n",
        "        continue\n",
        "\n",
        "    img = cv2.imread(full_img_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {full_img_path}\")\n",
        "        continue\n",
        "\n",
        "    faces = app.get(img)\n",
        "    preds = [list(map(int, face.bbox)) for face in faces if face.det_score > 0.5]\n",
        "\n",
        "    if len(preds) == 0 and len(gts) == 0:\n",
        "        continue\n",
        "\n",
        "    matched = set()\n",
        "    for p in preds:\n",
        "        hit = False\n",
        "        for idx, g in enumerate(gts):\n",
        "            if idx in matched:\n",
        "                continue\n",
        "            if iou(p, g) >= IOU_THRESH:\n",
        "                TP += 1\n",
        "                matched.add(idx)\n",
        "                hit = True\n",
        "                break\n",
        "        if not hit:\n",
        "            FP += 1\n",
        "    FN += len(gts) - len(matched)\n",
        "    image_count += 1\n",
        "\n",
        "# STEP 6: Results\n",
        "precision = TP / (TP + FP + 1e-9)\n",
        "recall = TP / (TP + FN + 1e-9)\n",
        "f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "\n",
        "print(\"\\nâœ… SCRFD on WIDER FACE (First 100 Images) Results:\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "SjRcyhMiAYJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***--CELEBA DATASET--***"
      ],
      "metadata": {
        "id": "_NCG8p69BGW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Limiting CELEBA Dataset to first 3000 images\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "bbox_file = \"/kaggle/input/celeba-dataset/list_bbox_celeba.csv\"\n",
        "save_dir = \"/kaggle/working/celeba_haar_preds\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load Haar Cascade\n",
        "haar = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Parameters\n",
        "scaleFactor = 1.1\n",
        "minNeighbors = 5\n",
        "minSize = (30, 30)\n",
        "\n",
        "# Limit to 3000 images\n",
        "img_paths = sorted(glob.glob(f\"{img_dir}/*.jpg\"))[:3000]\n",
        "\n",
        "# Run detection and save in YOLO format\n",
        "for img_path in tqdm(img_paths):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = haar.detectMultiScale(gray, scaleFactor=scaleFactor, minNeighbors=minNeighbors, minSize=minSize)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    with open(os.path.join(save_dir, f\"{name}.txt\"), \"w\") as f:\n",
        "        for (x, y, bw, bh) in faces:\n",
        "            if bw < 10 or bh < 10:\n",
        "                continue\n",
        "            xc = (x + bw/2) / w\n",
        "            yc = (y + bh/2) / h\n",
        "            bw_norm = bw / w\n",
        "            bh_norm = bh / h\n",
        "            f.write(f\"0 {xc:.6f} {yc:.6f} {bw_norm:.6f} {bh_norm:.6f}\\n\")\n"
      ],
      "metadata": {
        "id": "li6Eb2TdBGBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating YOLO Type Labels for Annotation\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PART 1: Defining Paths\n",
        "bbox_file = \"/kaggle/input/celeba-dataset/list_bbox_celeba.csv\"\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "label_save_dir = \"/kaggle/working/celeba_yolo_labels\"\n",
        "os.makedirs(label_save_dir, exist_ok=True)\n",
        "\n",
        "# PART 2: Reading bounding box CSV\n",
        "df = pd.read_csv(bbox_file, skiprows=1)  # Skip the header description row\n",
        "df.columns = [\"image_id\", \"x\", \"y\", \"w\", \"h\"]\n",
        "df = df[:3000]  # Only use first 3000 images\n",
        "\n",
        "created = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    filename = row[\"image_id\"]\n",
        "    x, y, w, h = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
        "\n",
        "    # Skip invalid boxes\n",
        "    if w <= 0 or h <= 0:\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(img_dir, filename)\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        img_w, img_h = img.size\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # Skip corrupted images\n",
        "    if img_w == 0 or img_h == 0:\n",
        "        continue\n",
        "\n",
        "    # Convert to YOLO format\n",
        "    x_center = (x + w / 2) / img_w\n",
        "    y_center = (y + h / 2) / img_h\n",
        "    w_norm = w / img_w\n",
        "    h_norm = h / img_h\n",
        "\n",
        "    # Apply epsilon *before* checking bounds\n",
        "    epsilon = 1e-6\n",
        "    x_center = (x + w / 2) / img_w\n",
        "    y_center = (y + h / 2) / img_h\n",
        "    w_norm = w / img_w\n",
        "    h_norm = h / img_h\n",
        "\n",
        "    x_center = min(max(x_center, 0), 1 - epsilon)\n",
        "    y_center = min(max(y_center, 0), 1 - epsilon)\n",
        "    w_norm = min(max(w_norm, 0), 1 - epsilon)\n",
        "    h_norm = min(max(h_norm, 0), 1 - epsilon)\n",
        "\n",
        "    # Now check bounds\n",
        "    if not (0 <= x_center < 1 and 0 <= y_center < 1 and 0 <= w_norm < 1 and 0 <= h_norm < 1):\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "    label_path = os.path.join(label_save_dir, filename.replace(\".jpg\", \".txt\"))\n",
        "    with open(label_path, \"w\") as f:\n",
        "        f.write(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
        "        created += 1\n",
        "\n",
        "print(f\"YOLO labels created: {created}\")\n",
        "print(\"Sample label files:\", sorted(os.listdir(label_save_dir))[:5])\n",
        "\n",
        "\n",
        "# PART 3: Visualization Function\n",
        "def visualize_detections(img_path, label_path, detected_boxes):\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # Draw GT boxes (blue)\n",
        "    if os.path.exists(label_path):\n",
        "      with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            print(f\"Label: {line.strip()}\")\n",
        "            _, xc, yc, bw, bh = map(float, line.strip().split())\n",
        "            x1 = int((xc - bw / 2) * w)\n",
        "            y1 = int((yc - bh / 2) * h)\n",
        "            x2 = int((xc + bw / 2) * w)\n",
        "            y2 = int((yc + bh / 2) * h)\n",
        "            print(f\"GT box pixel coords: ({x1}, {y1}) to ({x2}, {y2}) on image {w}x{h}\")\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "    # Draw detections (green)\n",
        "    for box in detected_boxes:\n",
        "        x, y, bw, bh = box\n",
        "        cv2.rectangle(img, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Green = Detection, Blue = Ground Truth\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# PART 4: Example: Visualize for 000003.jpg\n",
        "example_image = \"000003.jpg\"\n",
        "detected_boxes = [[50, 60, 80, 100]]  # <-- placeholder detection box\n",
        "visualize_detections(\n",
        "    os.path.join(img_dir, example_image),\n",
        "    os.path.join(label_save_dir, example_image.replace(\".jpg\", \".txt\")),\n",
        "    detected_boxes\n",
        ")\n"
      ],
      "metadata": {
        "id": "t09mgBVWBFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions:\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred_dir, label_dir, iou_thresh=0.5):\n",
        "    def parse_yolo_txt(file_path):\n",
        "        boxes = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    _, xc, yc, w, h = map(float, parts)\n",
        "                    boxes.append([xc, yc, w, h])\n",
        "        return boxes\n",
        "\n",
        "    def yolo_to_xyxy(box, img_w, img_h):\n",
        "        xc, yc, w, h = box\n",
        "        x1 = int((xc - w / 2) * img_w)\n",
        "        y1 = int((yc - h / 2) * img_h)\n",
        "        x2 = int((xc + w / 2) * img_w)\n",
        "        y2 = int((yc + h / 2) * img_h)\n",
        "        return [x1, y1, x2, y2]\n",
        "\n",
        "    def iou(boxA, boxB):\n",
        "        xA = max(boxA[0], boxB[0])\n",
        "        yA = max(boxA[1], boxB[1])\n",
        "        xB = min(boxA[2], boxB[2])\n",
        "        yB = min(boxA[3], boxB[3])\n",
        "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "        areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "        areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "        union = areaA + areaB - inter\n",
        "        return inter / union if union > 0 else 0\n",
        "\n",
        "    tp = fp = fn = 0\n",
        "    common_files = sorted(set(os.listdir(pred_dir)) & set(os.listdir(label_dir)))\n",
        "    img_size = (178, 218)  # CELEBA image size\n",
        "\n",
        "    for fname in common_files:\n",
        "        preds = parse_yolo_txt(os.path.join(pred_dir, fname))\n",
        "        gts = parse_yolo_txt(os.path.join(label_dir, fname))\n",
        "        preds = [yolo_to_xyxy(box, *img_size) for box in preds]\n",
        "        gts = [yolo_to_xyxy(box, *img_size) for box in gts]\n",
        "\n",
        "        matched = set()\n",
        "        for p in preds:\n",
        "            match_found = False\n",
        "            for idx, g in enumerate(gts):\n",
        "                if idx in matched:\n",
        "                    continue\n",
        "                if iou(p, g) >= iou_thresh:\n",
        "                    tp += 1\n",
        "                    matched.add(idx)\n",
        "                    match_found = True\n",
        "                    break\n",
        "            if not match_found:\n",
        "                fp += 1\n",
        "        fn += len(gts) - len(matched)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall = tp / (tp + fn + 1e-9)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "    return tp, fp, fn, precision, recall, f1"
      ],
      "metadata": {
        "id": "er2XdNb3CI0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***HAAR CASCADES (FRONTAL + SIDE) + NMS ON CELEBA***"
      ],
      "metadata": {
        "id": "70rRKVtmCJSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PART 1: Define Paths\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "save_dir = \"/kaggle/working/celeba_haar_combined_nms_preds\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# PART 2: Define Haar cascade models\n",
        "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
        "\n",
        "# PART 3: Define Parameters\n",
        "CONF_THRESH = 0.5\n",
        "IOU_THRESH = 0.3\n",
        "\n",
        "# PART 4: Apply NMS\n",
        "def apply_nms(boxes, scores, iou_thresh=0.3):\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    boxes_xywh = [[int(x), int(y), int(w), int(h)] for (x, y, w, h) in boxes]\n",
        "    indices = cv2.dnn.NMSBoxes(boxes_xywh, scores, score_threshold=CONF_THRESH, nms_threshold=iou_thresh)\n",
        "\n",
        "    # Flatten indices if necessary\n",
        "    if isinstance(indices, np.ndarray):\n",
        "        indices = indices.flatten()\n",
        "    elif isinstance(indices, tuple) and len(indices) == 1 and isinstance(indices[0], (list, np.ndarray)):\n",
        "        indices = indices[0]\n",
        "    elif isinstance(indices, list) and isinstance(indices[0], list):\n",
        "        indices = [i[0] for i in indices]\n",
        "\n",
        "    return [boxes[i] for i in indices]\n",
        "\n",
        "\n",
        "# PART 5: Run detection on first 3000 images\n",
        "img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])[:3000]\n",
        "\n",
        "for fname in tqdm(img_files):\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces with proper keyword arguments\n",
        "    frontal_faces = frontal_cascade.detectMultiScale(\n",
        "        image=gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=4,\n",
        "        minSize=(30, 30)\n",
        "    )\n",
        "    profile_faces = profile_cascade.detectMultiScale(\n",
        "        image=gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=4,\n",
        "        minSize=(30, 30)\n",
        "    )\n",
        "\n",
        "    # Combine results and apply NMS\n",
        "    all_faces = list(frontal_faces) + list(profile_faces)\n",
        "    scores = [0.99] * len(all_faces)\n",
        "    final_faces = apply_nms(all_faces, scores, iou_thresh=IOU_THRESH)\n",
        "\n",
        "    # Save detections in YOLO format\n",
        "    h, w = img.shape[:2]\n",
        "    txt_path = os.path.join(save_dir, fname.replace(\".jpg\", \".txt\"))\n",
        "    with open(txt_path, \"w\") as f:\n",
        "        for (x, y, w_box, h_box) in final_faces:\n",
        "            if w_box < 10 or h_box < 10:\n",
        "                continue\n",
        "            x_center = (x + w_box / 2) / w\n",
        "            y_center = (y + h_box / 2) / h\n",
        "            w_norm = w_box / w\n",
        "            h_norm = h_box / h\n",
        "            f.write(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
        "\n",
        "print(\"Haar cascade (frontal + profile) with NMS completed.\")\n",
        "print(\"Sample prediction files:\", sorted(os.listdir(save_dir))[:5])\n",
        "\n",
        "pred_dir = \"/kaggle/working/celeba_haar_combined_nms_preds\"\n",
        "label_dir = \"/kaggle/working/celeba_yolo_labels\"\n",
        "\n",
        "# PART 6: Compute metrics\n",
        "tp, fp, fn, precision, recall, f1 = compute_metrics(pred_dir, label_dir)\n",
        "\n",
        "# PART 7: Display results\n",
        "print(\"Haar Cascade Evaluation on CELEBA (first 3000 images, frontal+profile+NMS)\")\n",
        "print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "PSAJQNMVCfFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOV8n-FACE (without NMS) on CELEBA**"
      ],
      "metadata": {
        "id": "XoCE9B7bDf52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# PART 1: Define Paths\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "label_dir = \"/kaggle/working/celeba_yolo_labels\"  # already created GT labels\n",
        "model_path = \"/root/.cache/huggingface/hub/models--arnabdhar--YOLOv8-Face-Detection/snapshots/52fa54977207fa4f021de949b515fb19dcab4488/model.pt\"\n",
        "\n",
        "# PART 2: Load Model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# PART 3: Evaluation Parameters\n",
        "IOU_THRESH = 0.3\n",
        "CONF_THRESH = 0.2\n",
        "MIN_SIZE = 20\n",
        "\n",
        "TP = FP = FN = 0\n",
        "\n",
        "# PART 4: IoU Function\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# PART 5: Loop through first 3000 images\n",
        "img_files = sorted(os.listdir(img_dir))[:3000]\n",
        "\n",
        "for fname in tqdm(img_files):\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    label_path = os.path.join(label_dir, fname.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        continue  # Skip if no ground truth label\n",
        "\n",
        "    # Read GT boxes\n",
        "    gt_boxes = []\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = list(map(float, line.strip().split()[1:]))\n",
        "            xc, yc, w, h = parts\n",
        "            img = cv2.imread(img_path)\n",
        "            h_img, w_img = img.shape[:2]\n",
        "            x1 = int((xc - w / 2) * w_img)\n",
        "            y1 = int((yc - h / 2) * h_img)\n",
        "            w_box = int(w * w_img)\n",
        "            h_box = int(h * h_img)\n",
        "            gt_boxes.append([x1, y1, w_box, h_box])\n",
        "\n",
        "    # Run YOLOv8-Face prediction\n",
        "    results = model.predict(img, conf=CONF_THRESH, verbose=False)\n",
        "    preds = []\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "            conf = box.conf[0].item()\n",
        "            if conf < CONF_THRESH:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            if w < MIN_SIZE or h < MIN_SIZE:\n",
        "                continue\n",
        "            preds.append([int(x1), int(y1), int(w), int(h)])\n",
        "\n",
        "    # Match predictions to ground truth\n",
        "    matched = set()\n",
        "    for p in preds:\n",
        "        matched_flag = False\n",
        "        for i, g in enumerate(gt_boxes):\n",
        "            if i in matched:\n",
        "                continue\n",
        "            if iou(p, g) >= IOU_THRESH:\n",
        "                TP += 1\n",
        "                matched.add(i)\n",
        "                matched_flag = True\n",
        "                break\n",
        "        if not matched_flag:\n",
        "            FP += 1\n",
        "    FN += len(gt_boxes) - len(matched)\n",
        "\n",
        "# PART 6: Calculate Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"YOLOv8-Face Evaluation Results on CELEBA (first 3000 images):\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "3gickwgRDiVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOV8n-FACE (with NMS) on CELEBA**"
      ],
      "metadata": {
        "id": "NyMbnIdiEZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# STEP 1: Define Paths\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "label_dir = \"/kaggle/working/celeba_yolo_labels\"\n",
        "model_path = \"/root/.cache/huggingface/hub/models--arnabdhar--YOLOv8-Face-Detection/snapshots/52fa54977207fa4f021de949b515fb19dcab4488/model.pt\"\n",
        "\n",
        "# STEP 2: Load Model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# STEP 3: Evaluation Parameters\n",
        "IOU_THRESH = 0.3\n",
        "CONF_THRESH = 0.2\n",
        "MIN_SIZE = 20\n",
        "USE_NMS = True  # Optional NMS toggle\n",
        "NMS_THRESH = 0.2\n",
        "TP = FP = FN = 0\n",
        "\n",
        "# STEP 4: IoU Function\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# STEP 5: Apply NMS\n",
        "def apply_nms(boxes, scores, iou_thresh=0.4):\n",
        "    if not boxes:\n",
        "        return []\n",
        "    boxes_xywh = [[x, y, w, h] for (x, y, w, h) in boxes]\n",
        "    boxes_xyxy = [[x, y, x + w, y + h] for (x, y, w, h) in boxes]\n",
        "    indices = cv2.dnn.NMSBoxes(boxes_xyxy, scores, score_threshold=0.001, nms_threshold=iou_thresh)\n",
        "    if len(indices) == 0:\n",
        "        return []\n",
        "    indices = indices.flatten()\n",
        "    return [boxes[i] for i in indices]\n",
        "\n",
        "# STEP 6: Loop through first 3000 images\n",
        "img_files = sorted(os.listdir(img_dir))[:3000]\n",
        "\n",
        "for fname in tqdm(img_files):\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    label_path = os.path.join(label_dir, fname.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        continue\n",
        "\n",
        "    # Load GT labels\n",
        "    gt_boxes = []\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            xc, yc, w, h = map(float, line.strip().split()[1:])\n",
        "            img = cv2.imread(img_path)\n",
        "            h_img, w_img = img.shape[:2]\n",
        "            x1 = int((xc - w / 2) * w_img)\n",
        "            y1 = int((yc - h / 2) * h_img)\n",
        "            w_box = int(w * w_img)\n",
        "            h_box = int(h * h_img)\n",
        "            gt_boxes.append([x1, y1, w_box, h_box])\n",
        "\n",
        "    # YOLOv8 prediction\n",
        "    results = model.predict(img, conf=CONF_THRESH, verbose=False)\n",
        "    boxes = []\n",
        "    scores = []\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "            conf = box.conf[0].item()\n",
        "            if conf < CONF_THRESH:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            if w < MIN_SIZE or h < MIN_SIZE:\n",
        "                continue\n",
        "            boxes.append([int(x1), int(y1), int(w), int(h)])\n",
        "            scores.append(conf)\n",
        "\n",
        "    # Optional NMS\n",
        "    preds = apply_nms(boxes, scores, NMS_THRESH) if USE_NMS else boxes\n",
        "\n",
        "    # Match predictions to ground truth\n",
        "    matched = set()\n",
        "    for p in preds:\n",
        "        matched_flag = False\n",
        "        for i, g in enumerate(gt_boxes):\n",
        "            if i in matched:\n",
        "                continue\n",
        "            if iou(p, g) >= IOU_THRESH:\n",
        "                TP += 1\n",
        "                matched.add(i)\n",
        "                matched_flag = True\n",
        "                break\n",
        "        if not matched_flag:\n",
        "            FP += 1\n",
        "    FN += len(gt_boxes) - len(matched)\n",
        "\n",
        "# STEP 7: Calculate Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"YOLOv8-Face Evaluation Results on CELEBA (first 3000 images, NMS={USE_NMS}):\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "gaBB6Q2vEHKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCRFD on CELEBA**"
      ],
      "metadata": {
        "id": "KRIyo7YZE_cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PART 1: Define Paths\n",
        "model_path = \"/content/scrfd_10g_bnkps.onnx\"\n",
        "img_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
        "label_dir = \"/kaggle/working/celeba_yolo_labels\"\n",
        "\n",
        "# PART 2: Define Parameters\n",
        "CONF_THRESH = 0.5\n",
        "IOU_THRESH = 0.3\n",
        "NMS_THRESH = 0.4\n",
        "TP = FP = FN = 0\n",
        "model_input_size = (640, 640)\n",
        "\n",
        "# PART 3: Load SCRFD ONNX model\n",
        "session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "input_name = session.get_inputs()[0].name\n",
        "\n",
        "# PART 4: Preprocessing\n",
        "def preprocess(img, size=(896, 640)):\n",
        "    img_resized = cv2.resize(img, size)\n",
        "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
        "    img_norm = (img_rgb - 127.5) / 128.0\n",
        "    blob = img_norm.transpose(2, 0, 1)[None].astype(np.float32)\n",
        "    outputs = session.run(None, {input_name: blob})\n",
        "    print([o.shape for o in outputs])\n",
        "    return blob, img.shape[:2], size  # (orig_h, orig_w), input_size\n",
        "\n",
        "# PART 5: Apply NMS\n",
        "def nms(boxes, scores, threshold):\n",
        "    boxes_xywh = [[x, y, w, h] for (x, y, w, h) in boxes]\n",
        "    indices = cv2.dnn.NMSBoxes(boxes_xywh, scores, score_threshold=0.001, nms_threshold=threshold)\n",
        "    return [boxes[i[0]] for i in indices] if isinstance(indices, (list, np.ndarray)) and len(indices) > 0 else []\n",
        "\n",
        "# PART 6: Decode SCRFD Output\n",
        "def decode_output_scrfd_multiscale(outputs, orig_h, orig_w, conf_thresh, input_size=(640, 640)):\n",
        "    strides = [8, 16, 32]  # SCRFD default\n",
        "    num_levels = 3\n",
        "    input_w, input_h = input_size\n",
        "    scale_w = orig_w / input_w\n",
        "    scale_h = orig_h / input_h\n",
        "\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "\n",
        "    for i in range(num_levels):\n",
        "        scores = outputs[i]\n",
        "        bboxes = outputs[i + num_levels]\n",
        "\n",
        "        for idx in range(scores.shape[0]):\n",
        "            conf = float(scores[idx][0])\n",
        "            if conf < conf_thresh:\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = bboxes[idx]\n",
        "            x = int(x1 * scale_w)\n",
        "            y = int(y1 * scale_h)\n",
        "            w = int((x2 - x1) * scale_w)\n",
        "            h = int((y2 - y1) * scale_h)\n",
        "\n",
        "            boxes.append([x, y, w, h])\n",
        "            confidences.append(conf)\n",
        "\n",
        "    return boxes, confidences\n",
        "\n",
        "# PART 7: IoU\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = boxA[2] * boxA[3]\n",
        "    boxBArea = boxB[2] * boxB[3]\n",
        "    return interArea / float(boxAArea + boxBArea - interArea) if boxAArea + boxBArea - interArea > 0 else 0\n",
        "\n",
        "# PART 8: Main Evaluation Loop\n",
        "img_files = sorted(os.listdir(img_dir))[:3000]\n",
        "\n",
        "for fname in tqdm(img_files):\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    label_path = os.path.join(label_dir, fname.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        continue\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    blob, (orig_h, orig_w), input_size = preprocess(img, size=model_input_size)\n",
        "\n",
        "    # ONNX Inference\n",
        "    outputs = session.run(None, {input_name: blob})\n",
        "    boxes, scores = decode_output_scrfd_multiscale(outputs, orig_h, orig_w, CONF_THRESH, input_size)\n",
        "    print(f\"{fname} â†’ Detections: {len(boxes)}\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    for box in boxes:\n",
        "        x, y, w, h = box\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"{fname} â€” {len(boxes)} Detections\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    break  # visualize only first image\n",
        "\n",
        "\n",
        "    if boxes:\n",
        "        boxes = nms(boxes, scores, NMS_THRESH)\n",
        "\n",
        "    # Load GT\n",
        "    gt_boxes = []\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            xc, yc, w, h = map(float, line.strip().split()[1:])\n",
        "            x1 = int((xc - w / 2) * orig_w)\n",
        "            y1 = int((yc - h / 2) * orig_h)\n",
        "            w_box = int(w * orig_w)\n",
        "            h_box = int(h * orig_h)\n",
        "            gt_boxes.append([x1, y1, w_box, h_box])\n",
        "\n",
        "    # Match detections to GT\n",
        "    matched = set()\n",
        "    for pred in boxes:\n",
        "        found = False\n",
        "        for i, gt in enumerate(gt_boxes):\n",
        "            if i in matched:\n",
        "                continue\n",
        "            if iou(pred, gt) >= IOU_THRESH:\n",
        "                TP += 1\n",
        "                matched.add(i)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            FP += 1\n",
        "    FN += len(gt_boxes) - len(matched)\n",
        "\n",
        "# PART 9: Calculate Metrics\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(f\"SCRFD ONNX Evaluation on CELEBA (first 3000 images):\")\n",
        "print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "22qKSJljEE18"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}